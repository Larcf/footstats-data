name: Scrape Virtual Soccer Data

on:
  schedule:
    - cron: '*/15 * * * *'  # Executa a cada 15 minutos
  workflow_dispatch:        # Permite execução manual

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4
        pip install cloudscraper
        
    # Configurar proxy se necessário (descomente se precisar)
    # - name: Setup proxy
    #   run: |
    #     echo "HTTP_PROXY=http://proxy-server:port" >> $GITHUB_ENV
    #     echo "HTTPS_PROXY=http://proxy-server:port" >> $GITHUB_ENV
    #     echo "USE_PROXY=true" >> $GITHUB_ENV
        
    - name: Run scraper
      run: python scraping.py
      
    - name: Commit and push
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add live-matches.json
        git commit -m "Update match data" || echo "No changes to commit"
        git push
